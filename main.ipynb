{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c13b04e8",
   "metadata": {},
   "source": [
    "## Create VENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74281c0c",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python -m venv .controlnet\n",
    ". .controlnet/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67d586a",
   "metadata": {},
   "source": [
    "## Install Diffusers package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b1d992",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8a81e7",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "### Training Command\n",
    "\n",
    "Execute the training script with the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6758543e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png\n",
    "\n",
    "wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab9542",
   "metadata": {},
   "source": [
    "### 12GB VRAM GPU Configuration\n",
    "\n",
    "Add the following flags to the training command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae91e4e7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# ONE TIME ONLY: download the model and dataset to a local directory\n",
    "# incase no internet on training machine\n",
    "# on a machine with internet\n",
    "curl -LsSf https://hf.co/cli/install.sh | bash\n",
    "\n",
    "# this creates a local directory with all required files\n",
    "hf download stable-diffusion-v1-5/stable-diffusion-v1-5 \\\n",
    "  --local-dir shared/models/sd15 \n",
    "\n",
    "# on a machine with internet\n",
    "hf download fusing/fill50k \\\n",
    "  --repo-type dataset \\\n",
    "  --local-dir shared/datasets/fill50k\n",
    "\n",
    "# after downloading the datasets, extract the zip file and copy train.jsonl to metadata.jsonl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c747a63f",
   "metadata": {},
   "source": [
    "## Bash command - train_sd15.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbf69df",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=controlnet_fill50k\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=8\n",
    "#SBATCH --time=01:00:00\n",
    "#SBATCH --output=logs/controlnet_train_%j.log\n",
    "#SBATCH --error=logs/controlnet_train_%j.err\n",
    "\n",
    "export MODEL_DIR=\"/home/hpc/rlvl/rlvl165v/Desktop/controlnet/shared/models/sd15\"\n",
    "export DATASET_DIR=\"/home/hpc/rlvl/rlvl165v/Desktop/controlnet/shared/datasets/fill50k/extracted/\"\n",
    "export OUTPUT_DIR=\"/home/hpc/rlvl/rlvl165v/Desktop/controlnet/output/\"\n",
    "\n",
    "export HF_HOME=\"/home/woody/rlvl/rlvl165v/.cache/huggingface\"\n",
    "export HF_DATASETS_CACHE=\"$HF_HOME/datasets\"\n",
    "export TRANSFORMERS_CACHE=\"$HF_HOME/transformers\"\n",
    "export HF_HUB_CACHE=\"$HF_HOME/hub\"\n",
    "\n",
    "export HF_HUB_OFFLINE=1\n",
    "export TRANSFORMERS_OFFLINE=1\n",
    "export HF_DATASETS_OFFLINE=1\n",
    "\n",
    "accelerate launch train_controlnet.py \\\n",
    " --pretrained_model_name_or_path=$MODEL_DIR \\\n",
    " --output_dir=$OUTPUT_DIR \\\n",
    " --train_data_dir=\"$DATASET_DIR\" \\\n",
    " --image_column=\"image\" \\\n",
    " --conditioning_image_column=\"conditioning\" \\\n",
    " --caption_column=\"text\" \\\n",
    " --resolution=512 \\\n",
    " --learning_rate=1e-5 \\\n",
    " --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\" \\\n",
    " --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\n",
    " --train_batch_size=1 \\\n",
    " --gradient_accumulation_steps=4 \\\n",
    " --max_train_steps=50 \\\n",
    " --checkpointing_steps=10 \\\n",
    " --validation_steps=10 \\\n",
    " --gradient_checkpointing \\\n",
    " --use_8bit_adam \\\n",
    " --enable_xformers_memory_efficient_attention \\\n",
    " --set_grads_to_none\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23e1401",
   "metadata": {},
   "source": [
    "## Training with multiple GPUs\n",
    "\n",
    "`accelerate` allows for seamless multi-GPU training. Follow the instructions here for running distributed training with accelerate. Here is an example command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c829a9f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export MODEL_DIR=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "export OUTPUT_DIR=\"path to save model\"\n",
    "\n",
    "accelerate launch --mixed_precision=\"fp16\" --multi_gpu train_controlnet.py \\\n",
    " --pretrained_model_name_or_path=$MODEL_DIR \\\n",
    " --output_dir=$OUTPUT_DIR \\\n",
    " --dataset_name=fusing/fill50k \\\n",
    " --resolution=512 \\\n",
    " --learning_rate=1e-5 \\\n",
    " --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\" \\\n",
    " --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\n",
    " --train_batch_size=4 \\\n",
    " --mixed_precision=\"fp16\" \\\n",
    " --tracker_project_name=\"controlnet-test\" \\\n",
    " --report_to=wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7d0d2f",
   "metadata": {},
   "source": [
    "## Test with ADE20K dataset \n",
    "\n",
    "Condition reconstruction and FID score from paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c92c2f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#install the dataset tools package to download the dataset\n",
    "pip install --upgrade dataset-tools\n",
    "\n",
    "\n",
    "import dataset_tools as dtools\n",
    "\n",
    "dtools.download(dataset='ADE20K', dst_dir='~/dataset-ninja/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b6d2d6",
   "metadata": {},
   "source": [
    "## Performing inference with the trained ControlNet\n",
    "\n",
    "The trained model can be run the same as the original ControlNet pipeline with the newly trained ControlNet. Set `base_model_path` and `controlnet_path` to the values `--pretrained_model_name_or_path` and `--output_dir` were respectively set to in the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3eee3f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers.utils import load_image\n",
    "import torch\n",
    "\n",
    "base_model_path = \"/home/hpc/rlvl/rlvl165v/Desktop/controlnet/shared/models/sd15\"\n",
    "# Point directly to the output directory which contains config.json and safetensors file\n",
    "controlnet_path = \"/home/hpc/rlvl/rlvl165v/Desktop/controlnet/output/\"\n",
    "\n",
    "# Load with local_files_only=True to avoid trying to connect to huggingface.co\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    controlnet_path, \n",
    "    torch_dtype=torch.float16,\n",
    "    local_files_only=True\n",
    ")\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    base_model_path, \n",
    "    controlnet=controlnet, \n",
    "    torch_dtype=torch.float16,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# speed up diffusion process with faster scheduler and memory optimization\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "# remove following line if xformers is not installed or when using Torch 2.0.\n",
    "pipe.enable_xformers_memory_efficient_attention()\n",
    "# memory optimization.\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "control_image = load_image(\"./conditioning_image_1.png\")\n",
    "prompt = \"pale golden rod circle with old lace background\"\n",
    "\n",
    "# generate image\n",
    "generator = torch.manual_seed(0)\n",
    "image = pipe(\n",
    "    prompt, num_inference_steps=20, generator=generator, image=control_image\n",
    ").images[0]\n",
    "image.save(\"./output.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
